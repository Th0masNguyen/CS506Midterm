{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from os.path import exists\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from textblob import TextBlob\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Files\n",
    "\n",
    "Download the csv files into the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = pd.read_csv(\"./data/train.csv\")\n",
    "testingSet = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(\"train.csv shape is \", trainingSet.shape)\n",
    "print(\"test.csv shape is \", testingSet.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(trainingSet.head())\n",
    "print()\n",
    "print(testingSet.head())\n",
    "\n",
    "print()\n",
    "\n",
    "print(trainingSet.describe())\n",
    "\n",
    "trainingSet['Score'].value_counts().plot(kind='bar', legend=True, alpha=.5)\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print(\"EVERYTHING IS PROPERLY SET UP! YOU ARE READY TO START\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_normalize(df, exclude_columns=['Score', 'Id', 'Time'], method='minmax'):\n",
    "    # Separate the columns to exclude\n",
    "    excluded_df = df[exclude_columns] if all(col in df.columns for col in exclude_columns) else pd.DataFrame()\n",
    "\n",
    "    # Filter numeric columns excluding the specified columns\n",
    "    numeric_df = df.drop(columns=exclude_columns, errors='ignore').select_dtypes(include=['number'])\n",
    "\n",
    "    # Choose the normalization method\n",
    "    if method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'zscore':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method '{method}'\")\n",
    "\n",
    "    # Normalize the numeric columns\n",
    "    numeric_df_scaled = pd.DataFrame(scaler.fit_transform(numeric_df), columns=numeric_df.columns)\n",
    "\n",
    "    # Concatenate the excluded columns back with the normalized numeric columns\n",
    "    result_df = pd.concat([excluded_df.reset_index(drop=True), numeric_df_scaled], axis=1)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to(df):\n",
    "    # Initialize VADER Sentiment Analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # This is where you can do all your feature extraction\n",
    "    print(\"Starting feature extraction...\")\n",
    "\n",
    "    # Track progress for Helpfulness\n",
    "    print(\"Calculating Helpfulness...\")\n",
    "    df['Helpfulness'] = df['HelpfulnessNumerator'] / df['HelpfulnessDenominator']\n",
    "    df['Helpfulness'] = df['Helpfulness'].fillna(0)\n",
    "\n",
    "    # Track progress for ReviewLengthWords\n",
    "    print(\"Calculating ReviewLengthWords...\")\n",
    "    df['ReviewLengthWords'] = df['Text'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "\n",
    "    # Sentiment, Subjectivity, and VADER Sentiment with progress tracking\n",
    "    def get_sentiment_subjectivity_vader(text, index):\n",
    "        if isinstance(text, str):  # Check if the text is a string\n",
    "            # TextBlob for sentiment and subjectivity\n",
    "            blob = TextBlob(text)\n",
    "            sentiment = blob.sentiment.polarity  # Get sentiment polarity (-1 to 1)\n",
    "            subjectivity = blob.sentiment.subjectivity  # Get subjectivity (0 to 1)\n",
    "            \n",
    "            # VADER for additional sentiment analysis\n",
    "            vader_scores = sia.polarity_scores(text)\n",
    "            vader_compound = vader_scores['compound']  # VADER compound score (-1 to 1)\n",
    "        else:\n",
    "            sentiment = 0  # For null or non-string values\n",
    "            subjectivity = 0\n",
    "            vader_compound = 0\n",
    "\n",
    "        if index % 100000 == 0:  # Print progress every 100000 rows\n",
    "            print(f\"Processed {index} rows for sentiment, subjectivity, and VADER sentiment...\")\n",
    "        return sentiment, subjectivity, vader_compound\n",
    "\n",
    "    print(\"Calculating Sentiment, Subjectivity, and VADER Sentiment...\")\n",
    "    df[['Sentiment', 'Subjectivity', 'VaderSentiment']] = df.apply(\n",
    "        lambda row: pd.Series(get_sentiment_subjectivity_vader(row['Text'], row.name)), axis=1\n",
    "    )\n",
    "\n",
    "    print(\"Calculating Summary Sentiment, Subjectivity, and VADER Sentiment...\")\n",
    "    df[['SummarySentiment', 'SummarySubjectivity', 'SummaryVaderSentiment']] = df.apply(\n",
    "        lambda row: pd.Series(get_sentiment_subjectivity_vader(row['Summary'], row.name)), axis=1\n",
    "    )\n",
    "\n",
    "    # Adding mean ProductScore and mean UserScore\n",
    "    print(\"Calculating meanProductScore and meanUserScore...\")\n",
    "\n",
    "    # Calculate mean score for each ProductId, ignoring missing scores\n",
    "    mean_product_score = df.groupby('ProductId')['Score'].mean()\n",
    "\n",
    "    # Calculate mean score for each UserId, ignoring missing scores\n",
    "    mean_user_score = df.groupby('UserId')['Score'].mean()\n",
    "\n",
    "    # Merge these means back into the dataframe\n",
    "    df = df.merge(mean_product_score.rename('meanProductScore'), on='ProductId', how='left')\n",
    "    df = df.merge(mean_user_score.rename('meanUserScore'), on='UserId', how='left')\n",
    "\n",
    "    print(\"Feature extraction complete.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load the feature extracted files if they've already been generated\n",
    "if exists('./data/X_train.csv'):\n",
    "    X_train = pd.read_csv(\"./data/X_train.csv\")\n",
    "if exists('./data/X_submission.csv'):\n",
    "    X_submission = pd.read_csv(\"./data/X_submission.csv\")\n",
    "\n",
    "else:\n",
    "    # Process the DataFrame\n",
    "    train = add_features_to(trainingSet)\n",
    "    # train = filter_and_normalize(train)\n",
    "    # print(\"normalized & filtered train = \")\n",
    "    print(train.head())\n",
    "    print()\n",
    "    print(train.describe())\n",
    "\n",
    "    # Merge on Id so that the submission set can have feature columns as well\n",
    "    X_submission = pd.merge(train, testingSet, left_on='Id', right_on='Id')\n",
    "    X_submission = X_submission.drop(columns=['Score_x'])\n",
    "    X_submission = X_submission.rename(columns={'Score_y': 'Score'})\n",
    "\n",
    "    # The training set is where the score is not null\n",
    "    X_train =  train[train['Score'].notnull()]\n",
    "\n",
    "    X_submission.to_csv(\"./data/X_submission.csv\", index=False)\n",
    "    X_train.to_csv(\"./data/X_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnalyzeFeature(df, feature):\n",
    "    # Group by 'Score' and calculate the average of the specified feature for each score\n",
    "    avg_feature_by_score = df.groupby('Score')[feature].mean()\n",
    "\n",
    "    print(f\"Average {feature} by Score:\")\n",
    "    print(avg_feature_by_score)\n",
    "\n",
    "    # Plot the average of the specified feature by score\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    avg_feature_by_score.plot(kind='bar')\n",
    "    plt.title(f'Average {feature} by Score')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel(f'Average {feature}')\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation between the specified feature and 'Score'\n",
    "    correlation = df[[feature, 'Score']].corr()\n",
    "\n",
    "    print(f\"Correlation between {feature} and Score:\")\n",
    "    print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = [\"Time\", \"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"Helpfulness\", \"ReviewLengthWords\", \"Sentiment\", \"Subjectivity\", \"VaderSentiment\", 'SummarySentiment', 'SummarySubjectivity', 'SummaryVaderSentiment', 'meanProductScore', 'meanUserScore']\n",
    "for ft in Features:\n",
    "  AnalyzeFeature(X_train, ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample + Split into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out num-numeric columns\n",
    "Features = [\"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"Helpfulness\", \"ReviewLengthWords\", \"Sentiment\", \"Subjectivity\", \"VaderSentiment\", 'SummarySentiment', 'SummarySubjectivity', 'SummaryVaderSentiment', 'meanProductScore', 'meanUserScore']\n",
    "\n",
    "X_train = X_train[Features + [\"Score\"]]\n",
    "X_train = X_train.fillna(0)  # or use any appropriate method to handle NaNs\n",
    "\n",
    "# Split training set into training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_train.drop(columns=['Score']),\n",
    "    X_train['Score'],\n",
    "    test_size=1/4.0,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Print the initial distribution of Y_train\n",
    "print(\"Original class distribution:\")\n",
    "print(Y_train.value_counts())\n",
    "\n",
    "# Apply SMOTE for oversampling the minority classes\n",
    "# smote = SMOTE(random_state=0)\n",
    "# X_train_balanced, Y_train_balanced = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# # Print the new distribution of Y_train after balancing\n",
    "# print(\"Balanced class distribution:\")\n",
    "# print(Y_train_balanced.value_counts())\n",
    "\n",
    "# # Reduce the size of X_train and Y_train (for faster testing)\n",
    "# X_train_balanced = X_train_balanced.sample(frac=0.1, random_state=0)  # Use 10% of the balanced training set\n",
    "# Y_train_balanced = Y_train_balanced.loc[X_train_balanced.index]  # Ensure the labels match the reduced training set\n",
    "\n",
    "# X_train = X_train.sample(frac=0.25, random_state=0)  # Use 10% of the balanced training set\n",
    "# Y_train = Y_train.loc[X_train.index]  # Ensure the labels match the reduced training set\n",
    "\n",
    "# Print the size of the reduced dataset and a sample\n",
    "print(f\"Reduced size: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"Helpfulness\", \"ReviewLengthWords\", \"Sentiment\", \"Subjectivity\", \"VaderSentiment\", 'SummarySentiment', 'SummarySubjectivity', 'SummaryVaderSentiment', 'meanProductScore', 'meanUserScore']\n",
    "# features = ['Sentiment', 'VaderSentiment', 'SummaryVaderSentiment', 'SummarySentiment',\"HelpfulnessNumerator\"]\n",
    "\n",
    "X_test_select = X_test[features]\n",
    "X_submission_select = X_submission[features]\n",
    "\n",
    "X_train_select = X_train[features]\n",
    "Y_train_select = Y_train\n",
    "# X_train_select = X_train_balanced[features]\n",
    "# Y_train_select = Y_train_balanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_custom_thresholds(model, X_test, thresholds):\n",
    "    # Get the predicted probabilities for each class\n",
    "    probabilities = model.predict_proba(X_test)\n",
    "    \n",
    "    # Initialize an empty list to store predictions\n",
    "    predictions = []\n",
    "\n",
    "    # Loop through each sample in the test set\n",
    "    for i in range(len(probabilities)):\n",
    "        predicted_class = int(np.argmax(probabilities[i]))  # Default to the highest probability\n",
    "        \n",
    "        # Loop through each class and apply the custom thresholds\n",
    "        for j in range(len(thresholds)):\n",
    "            if probabilities[i][j] >= thresholds[j]:\n",
    "                predicted_class = j  # Update predicted class based on threshold\n",
    "                break\n",
    "                \n",
    "        predictions.append(predicted_class+1)  # Append the predicted class\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the model\n",
    "# model = KNeighborsClassifier(n_neighbors=9).fit(X_train_select, Y_train_select)\n",
    "# m2 = LogisticRegression(max_iter=300, solver='saga').fit(X_train_select, Y_train_select)\n",
    "# m4 = RandomForestClassifier(n_estimators=200, n_jobs=2).fit(X_train_select, Y_train_select)\n",
    "# m5 = SVC(C=4, kernel='sigmoid').fit(X_train_select, Y_train_select)\n",
    "m7 = GradientBoostingClassifier(learning_rate=0.50, n_estimators=300, random_state=0).fit(X_train_select, Y_train_select)\n",
    "\n",
    "\n",
    "# Predict the score using the model\n",
    "# Y_test_predictions = model.predict(X_test_select)\n",
    "# y2 = m2.predict(X_test_select)\n",
    "# y4 = m4.predict(X_test_select)\n",
    "# y5 = m5.predict(X_test_select)\n",
    "y7 = m7.predict(X_test_select)\n",
    "# Custom thresholds for classes 1, 2, 3, 4, 5\n",
    "# custom_thresholds = [0.3, 0.2, 0.2, 0.3, 0.45]  # Adjust as necessary\n",
    "# Y_test_predictions_custom = predict_with_custom_thresholds(model, X_test_select, custom_thresholds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_thresholds = [0.5, 0.5, 0.5, 0.5, 0.1]  # Adjust as necessary\n",
    "# y7b = predict_with_custom_thresholds(m7, X_test_select, custom_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate your model on the testing set\n",
    "# accuracy = accuracy_score(Y_test, Y_test_predictions)\n",
    "# print(\"Accuracy on testing set = \", accuracy)\n",
    "# print()\n",
    "# i = 1\n",
    "# accs = [accuracy]\n",
    "# for y in [y2, y4, y5, y7]:\n",
    "#   a = accuracy_score(Y_test, y)\n",
    "#   accs.append(a)\n",
    "#   i+=1\n",
    "# print(accs)\n",
    "\n",
    "# # Plot a confusion matrix\n",
    "# cm = confusion_matrix(Y_test, Y_test_predictions, normalize='true')\n",
    "# sns.heatmap(cm, annot=True)\n",
    "# plt.title(f'Confusion matrix of the classifier\\nAccuracy: {accuracy:.3f}')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.show()\n",
    "\n",
    "# a4 = accuracy_score(Y_test, y4)\n",
    "a7= accuracy_score(Y_test, y7)\n",
    "# a7b = accuracy_score(Y_test, y7b)\n",
    "\n",
    "# # Plot a confusion matrix\n",
    "# cm = confusion_matrix(Y_test, y4, normalize='true')\n",
    "# sns.heatmap(cm, annot=True)\n",
    "# plt.title(f'Confusion matrix of the classifier\\nAccuracy: {a4:.3f}')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.show()\n",
    "\n",
    "# Plot a confusion matrix\n",
    "cm = confusion_matrix(Y_test, y7, normalize='true')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title(f'Confusion matrix of the GB classifier\\nAccuracy: {a7:.3f}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# cm = confusion_matrix(Y_test, y7b, normalize='true')\n",
    "# sns.heatmap(cm, annot=True)\n",
    "# plt.title(f'Confusion matrix of the GB classifier\\nAccuracy: {a7b:.3f}')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average of missing mean vals\n",
    "sub = X_submission_select.fillna(4.1)\n",
    "# Create the submission file\n",
    "X_submission['Score'] = m7.predict(sub)\n",
    "submission = X_submission[['Id', 'Score']]\n",
    "submission.to_csv(\"./data/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
